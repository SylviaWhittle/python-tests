{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: c:\\Users\\sylvi\\Documents\\GitKraken\\python-tests\\ML\\Shape Detection\n",
      "shape: circle\n",
      "[WindowsPath('circle/0.png'), WindowsPath('circle/1.png'), WindowsPath('circle/10.png')]\n",
      "shape: star\n",
      "[WindowsPath('star/0.png'), WindowsPath('star/1.png'), WindowsPath('star/10.png')]\n",
      "shape: triangle\n",
      "[WindowsPath('triangle/0.png'), WindowsPath('triangle/1.png'), WindowsPath('triangle/10.png')]\n",
      "shape: square\n",
      "[WindowsPath('square/0.png'), WindowsPath('square/1.png'), WindowsPath('square/10.png')]\n"
     ]
    }
   ],
   "source": [
    "print(f'cwd: {Path.cwd()}')\n",
    "\n",
    "def split_folder(folder):\n",
    "    all_files = Path(f'{folder}/').glob(f'*.png')\n",
    "    all_files = list(all_files)\n",
    "    print(list(all_files)[0:3])\n",
    "    random.shuffle(all_files)\n",
    "    split = 0.7\n",
    "    split_index = int(np.floor(len(all_files) * split))\n",
    "    training = all_files[0:split_index]\n",
    "    testing = all_files[split_index:len(all_files)]\n",
    "    return training, testing\n",
    "\n",
    "Path.mkdir(Path('./train'), exist_ok=True)\n",
    "Path.mkdir(Path('./validate'), exist_ok=True)\n",
    "\n",
    "# data = {}\n",
    "# data[shape] = split_folder(shape)\n",
    "for shape in ['circle', 'star', 'triangle', 'square']:\n",
    "    print(f'shape: {shape}')\n",
    "    for dataset in ['train', 'validate']:\n",
    "        Path.mkdir(Path(f'./{dataset}/{shape}'), exist_ok=True)\n",
    "    train, validate = split_folder(shape)\n",
    "    for file in train:\n",
    "        shutil.copy(file, f'./train/{shape}')\n",
    "    for file in validate:\n",
    "        shutil.copy(file, f'./validate/{shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circle', 'square', 'star', 'triangle']\n"
     ]
    }
   ],
   "source": [
    "image_files = datasets.ImageFolder('train', transform=transforms.ToTensor())\n",
    "data_loaders = torch.utils.data.DataLoader(image_files, batch_size=5, shuffle=True)\n",
    "class_name = image_files.classes\n",
    "print(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the image and labels\n",
    "data_dir = './'\n",
    "# Dictionary containing image info in both train and test\n",
    "image_files = {x:datasets.ImageFolder(os.path.join(data_dir, x), transform=transforms.ToTensor())\n",
    "                for x in [\"train\"]} #only using tensor transform for now\n",
    "# Dictionary containing the data loader\n",
    "data_loaders = {x:torch.utils.data.DataLoader(image_files[x], batch_size=5, shuffle=True) for x in \n",
    "               [\"train\"]}\n",
    "# Getting the class names\n",
    "class_name = image_files[\"train\"].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a simple CNN\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, \n",
    "                             stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, \n",
    "                              stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.cnn3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5,\n",
    "                             stride=1, padding=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1568, 600)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(600, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.cnn3(out)\n",
    "        out = self.bn3(out)\n",
    "        self.relu(out)\n",
    "        out = out.view(5, len(image_files)) # batch_size = 5\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, 400000]' is invalid for input of size 400000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     16\u001b[0m     iter_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\sylvi\\Documents\\GitKraken\\python-tests\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [24], line 36\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m---> 36\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400000\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# batch_size = 5\u001b[39;00m\n\u001b[0;32m     37\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(out)\n\u001b[0;32m     38\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[5, 400000]' is invalid for input of size 400000"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Training\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for images, labels in data_loaders['train']:\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        iter_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        iterations += 1\n",
    "    \n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    train_acc.append(100 * correct / len(image_files))\n",
    "    \n",
    "    print(\"Epoch [{}/{}], Training loss: {:.3f}, Training accuracy: {:.3f}\".format(epoch+1, epochs, train_loss[-1], train_acc[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe13cf6fe9ef099f61808b128c1ea8743fed4050323d22ae11717cfb12cdbfba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
